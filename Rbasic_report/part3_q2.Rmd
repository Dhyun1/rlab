---
title: "326.212 Final Project: Part 3 - Q2"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, error = TRUE, 
                      message = FALSE, warning = FALSE)
```

```{r packages, include = FALSE}
library(tidyverse)
library(MASS)
library(tmap)
library(sf)
library(lubridate)
library(corrplot)
library(dplyr)
library(modelr)
```

```{r part1 code, include = FALSE}
#getwd()
#dir.create("./data")

info<-read.csv("./data/Measurement_info.csv")
item<-read.csv("./data/Measurement_item_info.csv")
station<-read.csv("./data/Measurement_station_info.csv")

###
data<-info%>%
  full_join(item)%>%
  full_join(station)

###
data = data %>%
  dplyr::mutate(`Pollution level` = ifelse(Average.value<0,NA,ifelse(Average.value<=Good.Blue.,"Good(Blue)",         ifelse(Average.value<=Normal.Green.,"Normal(Green)",                          ifelse(Average.value<=Bad.Yellow.,"Bad(Yellow)",                        ifelse(Average.value<=Very.bad.Red.,"Very bad(Red)",NA))))))

###
data%>%
  dplyr::select(Item.name,Average.value,`Pollution level`)%>%
  head()
```

```{r part2 code, include = FALSE}
var_remove<-c("Station.code","Address","Latitude","Longitude","Item.code",   "Unit.of.measurement","Good.Blue.","Normal.Green.","Bad.Yellow.","Very.bad.Red.")   
index<-sapply(var_remove, function (x) return (which(x==names(data))))

data<-data[,-index]
names(data)

###
type<-sapply(names(data), function (x) return (typeof(data[,x]))) 

var_factor<-names(data)[which(type=="character")]
for (i in 1:length(var_factor)){
    data[,var_factor[i]]<-factor(data[,var_factor[i]])
}
str(data)

###
sum(is.na(data))
sapply(names(data),function(x){
  sum(is.na(data[,x]))
})
###
data%>%
  filter(Average.value<0)%>%
  dplyr::select(Average.value,Instrument.status)%>%
  count(Average.value,Instrument.status)
###
data0<-data%>%
  left_join(item)%>%
  filter(Average.value<=Very.bad.Red.) 
data1<-data%>%
  left_join(item)%>%
  filter(Average.value>Very.bad.Red.)%>%
  mutate(`Pollution level`="Very bad(Red)")

#rbind data0, data1 and select certain columns that question requires.
m<-rbind(data0,data1)%>%
  filter(Average.value>=0)%>%
  dplyr::select("Measurement.date","Average.value","Instrument.status","Item.name","Station.name.district.",`Pollution level`)

data<-m
head(data)
###
#frequency table
table(data$Instrument.status)
#proportion table
table(data$Instrument.status)/nrow(data)
#proportion plot
ggplot(data,aes(x=factor(Instrument.status)))+
  geom_bar(aes(y = (..count..)/sum(..count..)))+
  labs(x="Instrument Status",y="proportion",title="Proportion of Instrument Status") +
  theme_bw()

###
data%>%
  left_join(item)%>%
  filter(Instrument.status==9)%>%
  dplyr::select(Average.value,Very.bad.Red.)%>%
  head()#not that much abnormal data of Average value

data<-data%>%
  mutate(Instrument.status=ifelse(Instrument.status!=0,2,Instrument.status))
#consider all Instrument status with nonzero value(just as definition, means not normal) as abnormal(Instrument status=2)

ggplot(data,aes(x=factor(Instrument.status)))+
  geom_bar(aes(y = (..count..)/sum(..count..)))+
  labs(x="Instrument Status",y="proportion",title="Proportion of Instrument Status(nonzero value processed)") +
  theme_bw()

data_part2q4<-data

###
data%>%
  group_by(Item.name)%>%
  filter(Average.value==max(Average.value))%>%
  dplyr::select(Measurement.date,Station.name.district.)

###
data%>%
  ggplot()+
  geom_bar(aes(x=Item.name, fill=`Pollution level`),position = "fill")+
  labs(title="Levels of pollution in Seoul from 2017 to 2019", x="Item name",y="proportion")+
  theme(plot.title = element_text(hjust = 0))
###
head(data)
###
data%>%
  filter(Item.name=="PM2.5")%>%
  group_by(Station.name.district.)%>%
  summarise(
    PM2.5avg=mean(Average.value)
  )%>%
  filter(rank(PM2.5avg)%in%c(1,25))
###
PM2.5<-data%>%
  filter(Item.name=="PM2.5",Station.name.district.%in%c("Gangbuk-gu","Yeongdeungpo-gu"))%>%
  dplyr::select(Average.value,Station.name.district.)

var.test(PM2.5$Average.value~PM2.5$Station.name.district.)
#p-value<0.05, we say they have non-equal variance

t.test(PM2.5$Average.value~PM2.5$Station.name.district.,alternative="less",
       var.equal=FALSE,alpha=0.05)
###
ymdh<-data.frame(sapply(c("year","month","mday","hour"),function (x){
  get(x)(parse_datetime(as.character(data$Measurement.date)))
}))

data<-cbind(data,ymdh)%>% 
  dplyr::rename(Year=year,Month=month,Day=mday,Hour=hour)
head(data)
###
data%>%
  dplyr::select(Average.value,Item.name,Month)%>%
  filter(Item.name%in%c("PM2.5","PM10"))%>%
  group_by(Month)%>%
  summarise(
    avg25=mean(Average.value[which(Item.name=="PM2.5")]),
    avg10=mean(Average.value[which(Item.name=="PM10")])
  )%>%
  ggplot(aes(x=Month))+
  geom_line(aes(y=avg25),colour="green")+
  geom_line(aes(y=avg10),colour="blue")+
  labs(x="Month",y="Average value",title="Average value of PM2.5(blue), PM10(green)")+
  scale_x_continuous(breaks=seq(1,12,by=1))+
  theme_bw()
###
#dataframe(spread)
data.spread<-data%>%
  dplyr::select(Measurement.date,Station.name.district.,Item.name,Average.value)%>%
  tidyr::spread(key="Item.name",value="Average.value")%>%
  dplyr::select(CO:SO2)
#remove na rows
data.spread<-data.spread%>%
    filter(!is.na(CO),!is.na(SO2),!is.na(PM2.5),!is.na(PM10),!is.na(NO2),!is.na(O3))
head(data.spread)
sum(is.na(data.spread))
###
cor(data.spread)
##
corrplot::corrplot(cor(data.spread))
###
p_1<-data%>%
  filter(Item.name %in% c("NO2","O3"))%>%
  group_by(Month)%>%
  summarise(
    avgN02=mean(Average.value[which(Item.name=="NO2")]),
    avgO3=mean(Average.value[which(Item.name=="O3")])
  )%>%
  ggplot(aes(x=Month))+
  geom_line(aes(y=avgN02),colour="green")+
  geom_line(aes(y=avgO3),colour="blue")+
  labs(x="Month",y="Average value",title="Monthly Average value of NO2(blue), O3(green)",collapse="")+
  scale_x_continuous(breaks=seq(1,12,by=1))+
  theme_bw()

p_2<-data%>%
  filter(Item.name %in% c("NO2","O3"))%>%
  group_by(Hour)%>%
  summarise(
    avgN02=mean(Average.value[which(Item.name=="NO2")]),
    avgO3=mean(Average.value[which(Item.name=="O3")])
  )%>%
  ggplot(aes(x=Hour))+
  geom_line(aes(y=avgN02),colour="green")+
  geom_line(aes(y=avgO3),colour="blue")+
  labs(x="Hour",y="Average value",title="Hourly Average value of NO2(blue), O3(green)")+
  scale_x_continuous(breaks=seq(0,23,by=1))+
  theme_bw()
print(p_1)
print(p_2)
###
```

## 2020-10451 Baek Dae Hyun

## Part 3 - Q2

### Step 1

## [문제]

```{r}
##1##

w2017<-read.csv("./data/weather2017.csv")
w2018<-read.csv("./data/weather2018.csv")
w2019<-read.csv("./data/weather2019.csv")
x<-rbind(w2017,w2018,w2019)%>%
  rename("Station code"=1,"District"=2,"Measurement date"=3,"Temper"=4,"Direct"=5,"Speed"=6)
head(x)
```

rbind와 rename을 통해 문제 조건에 맞는 데이터 x를 생성하였다.

```{r}
##2##

w2017jo<-read.csv("./data/weather2017(Jongno).csv")
w2018jo<-read.csv("./data/weather2018(Jongno).csv")
w2019jo<-read.csv("./data/weather2019(Jongno).csv")
y<-rbind(w2017jo,w2018jo,w2019jo)%>%
  rename("Station code"=1,"District"=2,"Measurement date"=3,"Temper"=4,"Speed"=5,"Direct"=6)

head(y)
```

rbind와 rename을 통해 문제 조건에 맞는 데이터 y를 생성하였다.

```{r}
##3##
z<-rbind(x,y)
head(z)
```

1과 2에서 만든 x, y를 rbind하여 문제 조건에 맞는 데이터 z를 생성하였다.

```{r}
##4##
kma<-read.csv("./data/kmacode.csv")

z0<-z%>%
  rename(Station.code=`Station code`)%>%
  full_join(kma,by="Station.code")%>%
  rename(District=District.y)%>%
  dplyr::select(-District.x,-Station.code)

head(z0)

```

kma와 z는 station code를 key로 활용하였을 때 결합될 수 있다. 따라서 z의 변수명을 kma의 것과 같이 Station.code로 바꾸고 kma와 그 key로 결합한 뒤, kma에 들어있는 지역구의 영어이름을 District로 저장, 나머지 불필요한 열들은 모두 제거하여 최종적으로 기존 데이터의 지역구만 영어로 바뀐 데이터셋 z0를 만든다.


```{r}
##5##
z1<-data_part2q4%>%
  dplyr::select(Measurement.date,Station.name.district.,Item.name,Average.value)%>%
  tidyr::spread(key="Item.name",value="Average.value")%>%
rename(District=Station.name.district.,"Measurement date"=Measurement.date)

head(z1)
```

이 문제에서는 part 2의 Q4 처리가 끝난 데이터를 사용하라고 하였는데, 이를 위해 part 2 문제 진행을 할 때 Q4가 끝난 데이터를 data_part2q4로 따로 저장해놓았다. 문제는 part 2의 Q12 문제의 spread 과정과 동일하며, key와 value 값을 item name과 average value로 한 뒤 문제 조건에 맞게 약간의 열 이름의 수정을 가하여 데이터프레임 z1을 만들었다.

```{r}
##6##
head(z0)
head(z1)
step1<-inner_join(z0,z1,by=c("Measurement date","District"))%>%
  dplyr::select(`Measurement date`,District,CO,NO2,O3,PM10,PM2.5,SO2,Temper,Direct,Speed)
#print data that was made
head(step1)
```

공통된 Measurement와 District만을 병합해야 하므로 full_join이 아닌 inner_join을 수행한 뒤, 필요한 열이 아닌 불필요하게 join된 열은 모두 제거하였다.

### Step 2

## [문제 1] 재현

```{r}
#1
step2_PM10<-step1[!is.na(step1),]%>%
  filter(PM10>150)%>%
  dplyr::select(Direct)
head(step2_PM10)

```

filter와 select를 통해 문제 조건에 맞는 데이터 step2_PM10을 생성하였다.

```{r}
#2
step2_PM2.5<-step1[!is.na(step1),]%>%
  filter(PM2.5>75)%>%
  dplyr::select(Direct)
head(step2_PM2.5)
```

filter와 select를 통해 문제 조건에 맞는 데이터 step2_PM10을 생성하였다.

```{r}
#3
step2_PM10<-step2_PM10%>%
  filter(!is.na(Direct),Direct>0,Direct<=360)
step2_PM2.5<-step2_PM2.5%>%
  filter(!is.na(Direct),Direct>0,Direct<=360)

head(step2_PM10)
head(step2_PM2.5)
```

앞서 만든 2개의 데이터프레임에 똑같이 filter를 통해 문제 조건이 제시하는 범위에 속하는 Direct 값을 갖는 열만 추려내었다.


```{r}

#4
direction_8<-factor(c("N","NE","E","SE","S","SW","W","NW","N"))
breaks <- c(0, 360/16, (1/16 + (1:7 / 8)) * 360, 360)
#breaks = 0.0  22.5  67.5 112.5 157.5 202.5 247.5 292.5 337.5 360.0
k<-step2_PM10
l<-step2_PM2.5
step2_PM10<-step2_PM10%>%
  mutate(Direct=cut(Direct,breaks, labels=direction_8))
step2_PM2.5<-step2_PM2.5%>%
  mutate(Direct=cut(Direct,breaks, labels=direction_8))

head(step2_PM10)
head(step2_PM2.5)
```

direction_8이라는 변수에 8방위 변수를 모두 넣는다. 여기서 0부터 360까지 시작하면서 각 구간의 길이는 45로, 시작과 끝에 해당하는 N 방위는 0~22.5, 337.6~360의 2가지 범위에서 정의되므로 시작과 끝에 모두 넣었다. cut이라는 함수는 break라는 인자를 통해 구간 범위를 지정하면, 특정 값이 어떤 구간에 속하냐에 따라 해당하는 label이 출력되게 끔 할 수 있다. 따라서 labels를 8방위로 하고, input을 0~360도까지의 임의의 숫자를 넣어주면 45도 간격으로 배치되어 있는 구간들 중 어느 구간에 속해있는지 찾아 그것에 대응되는 방위를 표시할 수 있게 된다.

```{r}
#5
prop_PM10<-step2_PM10%>%
  group_by(Direct)%>%
  summarise(
    item="PM10",
    Proportion=n()/nrow(step2_PM10)
  )
prop_PM2.5<-step2_PM2.5%>%
  group_by(Direct)%>%
  summarise(
    item="PM2.5",
    Proportion=n()/nrow(step2_PM2.5)
  )

print(prop_PM10)
print(prop_PM2.5)
```

Direction에 따라 group_by하고 proportion을 summarise를 통해 정의해서 proportion 비율을 direction 별로 계산하여 출력될 수 있게 하였다.

```{r}
#6
prop<-full_join(prop_PM10,prop_PM2.5)

ggplot(prop,aes(x=factor(Direct), y=Proportion))+
  geom_point(size=2)+
  geom_polygon(aes(group=item,colour=item),fill=NA)+
  coord_polar(start=-pi/8)+
  labs(x="", y="Proportion")+
  theme_light()
```

5번에서 만든 데이터를 합쳐, 각 item 별로 방위의 비율을 나타낸 하나의 데이터프레임을 만든다. 이 데이터프레임에 ggplot + geom_point + geom_polygon + coord_polar을 이용하면 주어진 그림을 만들 수 있다. 


## [문제 2]  위 그림을 바탕으로 미세먼지 원인에 대한 결론을 제시, 이유 분석

위의 그림에 따르면, PM10과 PM2.5의 오염물질 양 NW,W,SW 방위에서 많았다. 즉 서쪽에서 불어오는 바람과 초미세먼지의 양은 강한 양의 상관관계임을 알 수 있다. 우리나라는 겨울에 북서풍 여름에는 남서풍이 강하다. 그리고 위 현상은 이러한 강력한 서풍의 영향과 관련이 깊은 것으로 보이며, 중국 지역에서 발생하는 미세먼지를 비롯한 다양한 종류의 오염물질이 서풍을 타고 우리나라로 넘어오기 때문이라고 해석할 수 있다.


## [문제 3] 위 과정을 보완하는 방법 제시, 그 근거 서술

위 그림에서 서풍 계열의 영향이 크다는 것을 확인하였는데, 정확히 NW, W, SW 중 어느 방위 방향이 제일 영향력이 큰지에 대해서도 확인해볼 필요가 있을 것 같다. 그 이유는 보통 일반인들이 생각하는 북서풍으로 인한 중국 사막과 공장발 미세먼지만을 생각하는데, SW 남서 방향도 NW와 견줄 정도로 영향력이 컸기 때문이다(오히려 그래프 상에는 SW가 NW보다 더 큰 값을 보였다).
실제로 2019~2020년 中 듀크쿤샨대 연구팀 분석결과에 따르면(동아사이언스 인용, http://dongascience.donga.com/news.php?idx=33474)
미세먼지 농도에 가장 큰 영향을 주는 바람의 방향으로는 남서풍이 지목됐다. 이는 중국 내 주요 공업 단지가 몰려 있어 기존에 우리나라의 미세먼지 농도에 가장 큰 영향을 줄 것으로 생각됐던 이른바 '징진지'(Jingjinji) 지역(베이징, 텐진, 허베이)과 사막이 몰려 있어 황사 근원지로 꼽히는 내몽고 쪽에서 불어오는 '북서풍'이 중국발 미세먼지의 원인이 아니라는 뜻이다. 따라서 북서풍과 남서풍 어느 것이 더 우리나라의 미세먼지 발생량에 영향을 주는지를 알아보는 것은 흥미로운 주제일 것이다. 이에 대하여 두 모집단의 평균 차이를 검정하는 t-test를 NW, SW에 대하여 모든 종류의 오염물질에 대하여 검정하여 NW와 SW 영향력의 차이가 유의하게 다른지 검정할 수 있을 것이다.
 또한 연구팀에 따르면 미세먼지의 중국발 영향은 여름·겨울·봄·가을 순 영향으로 크다고 한다. 따라서 앞서 한 분석을 계절에 따라 grouping하여 분석한다면 연구팀의 결과를 검증해볼 수 있어 본 연구에 더 의미를 부여할 수 있을 것이다.

### Step 3

## [문제 1] 재현, PM2.5와 PM10에 대하여 상관계수, 결정계수의 값?

```{r}

##1## 
#filtering
step3<-step1%>%
  filter(!is.na(Temper),!is.na(PM10),!is.na(PM2.5))%>%
  filter(month(parse_datetime(`Measurement date`)) %in% c(12,1,2))%>%
  filter(hour(parse_datetime(`Measurement date`)) %in% c(3:9))
```

filter 함수를 통해 문제 조건에 맞는 열만 추출된 dataframe step3를 만들었다.

```{r}
##2##
#add hour columns for summarising statistics by hour
step3_hour<-step3%>%
    mutate(Hour=hour(parse_datetime(`Measurement date`)))
#group by hour, district, and summarise mean and sd of pollutants
stat<-step3_hour%>%
  group_by(Hour,District)%>%
  summarise(
    meanT=mean(Temper),
    sdT=sd(Temper),
    mean10=mean(PM10),
    sd10=sd(PM10),
    mean2.5=mean(PM2.5),
    sd2.5=sd(PM2.5)
  )
names(stat)
```

시간대별, 지역별 Temper, PM10, PM2.5의 평균과 분산을 구하기 위해, 우선 step3 데이터프레임의 Measurement.date를 이용하여 hour 변수를 생성하고, group_by를 
hour과 district에 대하여 수행하고 summarise를 이용해 각 변수의 mean과 sd를 계산, 6개의 서로 다른 변수(meanT, sdT, mean10, sd10, mean2.5, sd2.5)에 저장하였다. 

```{r}
##3## 
#join two dataframes we've just made above. 
step3_stat<-full_join(step3_hour,stat,by=c("Hour","District"))
#standardize pollutants' average value.
step3_stan<-step3_stat%>%
  mutate(Temper=(Temper-meanT)/(sdT),PM10=(PM10-mean10)/(sd10),
         PM2.5=(PM2.5-mean2.5)/(sd2.5))%>%
  dplyr::select(`Measurement date`:Speed)

#substitute original data to standardized data.
step3<-step3_stan
head(step3)
```

문제 2에서 구한 평균과 표준편차 데이터를 step3에 hour 열이 추가된 step3_hour에 대하여 full_join하면 각 열은 그것의 hour과 district에 해당하는 Temper, PM2.5, PM10의 평균과 표준편차 값을 포함하게 된다. 따라서 mutate를 통해 이 값들을 통해 standardize를 쉽게 할 수 있고, 기존 데이터와 같은 형식을 취하기 위해 필요한 열만 select 함수로 뽑은 뒤 이 데이터를 기존의 step3에 새로 저장하였다. 

```{r}
##4##
fit_PM10<-lm(PM10~Temper, step3)
grid<-step3%>%
  data_grid(Temper)
grid <- grid %>% 
  modelr::add_predictions(fit_PM10)

ggplot(step3, aes(x=Temper)) +
  geom_point(aes(y = PM10),colour="red") +
  geom_line(aes(y = pred), data = grid)
#step3%>%
##  ggplot(aes(x=Temper,y=PM10))+
#  geom_point(colour="Red") +
#  stat_smooth(method = 'lm')
  

summary(fit_PM10) 
#Multiple R-squared:  0.09736,	Adjusted R-squared:  0.09734 
cor(step3$PM10,step3$Temper)
#correlation coeff: 0.3120
```

```{r}
##5##


#step3%>%
#  ggplot(aes(x=Temper,y=PM2.5))+
#  geom_point(colour="Blue") +
#  stat_smooth(method = 'lm')
  
fit_PM2.5<-lm(PM2.5~Temper, step3)
summary(fit_PM2.5) 

fit_PM2.5<-lm(PM2.5~Temper, step3)
grid<-step3%>%
  data_grid(Temper)
grid <- grid %>% 
  modelr::add_predictions(fit_PM2.5)

ggplot(step3, aes(x=Temper)) +
  geom_point(aes(y = PM2.5),colour="blue") +
  geom_line(aes(y = pred), data = grid)

#Multiple R-squared:  0.1488,	Adjusted R-squared:  0.1487 
cor(step3$PM2.5,step3$Temper)
#correlation coeff: 0.3857
```

다시 한번 각 문제의 풀이를 정리하면,
1번은 원래 데이터프레임(step3)의 filter를 이용해 간단히 구할 수 있다.

2번은 기존 데이터에 hour 변수를 추가한 데이터 프레임을 만든 뒤(step3_hour), 이것에 대해 group_by를 hour과 district 2가지 변수에 대해 적용, summarise 
함수로 Temp, PM10, PM2.5의 평균과 표준편차 값을 구하였다. 

3번은 2번에서 구한 값들을 하나의 데이터프레임(stat)으로 저장, 이것을
step3_hour에 대하여 key= hour과 district으로 하여 join하고, mutate를 통해 각 Temper, PM10, PM2.5에 그것들의 각 시간대, 지역별로 구해진 평균과 표준편차를 
이용해 표준화 작업을 한다. 표준화된 데이터 프레임 step3_stan을 기존의 데이터 프레임에 뒤집어 씌워 최종적으로 step3는 표준화된 데이터프레임이 되었다.

4번은 lm fit에 grid에 대한 prediction 값을 저장하고 이를 ggplot + geom_point + geom_line으로 산점도와 회귀선을 그릴 수 있으며 문제 요구 사항으로 geom_point의 color="Red"로 한다. 그 외에 ggplot 옵션은 기본 설정을 따른다.
상관계수와 결정계수를 구하려면 각각 cor(), summary(lm())을 실행해주면 된다. 상관계수는 0.3120으로 낮게 나타났고, R-squared 값은 0.09736(Multiple), 0.09734(Adjusted)로 나타났다.

5번은 4번과 마찬가지로 하되, 변수를 PM2.5로 바꾸고 color="Blue"로 하여 진행하면 되므로 결과만 요약하면 상관계수는 0.3857, R-squared 값은 0.1488(Multiple), 0.1487(Adjusted)로 나타났다.

## [문제 2] [문제 1]의 결과에 대한 결론 제시, 이유 분석

문제 1에 해당하는 분석 결과, 온도라는 동일 변수에 대하여 PM10과 PM2.5를 선형 회귀 적합시켰을 때 상관계수는 각각 0.3120, 0.3857로 PM2.5가 온도에 관한 상관계수가 높았다. 그리고 결정계수는 Adjusted R-squared 값으로 비교하면 (Multiple의 경우도 같은 결과를 제시한다) 0.09734와 0.1487로 나타났다. 이 경우도 역시 PM2.5가 PM10보다 온도에 대한 선형 회귀 적합에서 결정계수가 크게 나타났다. 전반적인 결과를 종합하면 PM2.5가 PM10보다 더 온도에 관한 선형모델을 적합했을 때 설명력이 좋다고 보여진다.  
 비록 회귀 모델은 PM2.5와 PM10 모두 유의하지만, 결정계수는 0.1 근방으로 매우 낮은 수준이다. 따라서 선형 모델 방법을 적합하였을 때는 온도와 PM10, PM2.5의 관계를 잘 설명하지 못한다고 보여지며 그 이유는 선형 모델은 기본적으로 두 변수간 선형성(선형 상관관계)를 바탕으로 하고 있지만 실제 사회 현상은 선형성을 띠기보다는 복잡한 사이 매개변수, 왜곡변수로 인하여 더 복잡한 모델을 요구한다. 실제로, 서울 대도시의 건물의 높이가 높다는 점과 높은 매연 발생량 등은 열섬현상과 더스트 돔 현상과 양의 상관관계가 있고 이것은 미세먼지 발생량과 온도 사이에 매개변수로 작용할 것이다. 따라서 단지 온도와 미세먼지라는 두 변수사이의 관계를 분석할 것이 아니라, 다양한 매개변수 왜곡변수 등을 회귀식에 고려하여 선형 모델을 분석하여야 할 것이다. 
 

## [문제 3] 위 과정을 보완하는 방법 제시, 그 근거 서술

```{r}
summary(fit_PM10)
summary(fit_PM2.5)

PM10.res = resid(fit_PM10)
PM2.5.res = resid(fit_PM2.5)
plot(PM10.res)
abline(0,0) 

plot(PM2.5.res)
abline(0,0) 
```

문제 2의 논의처럼 회귀 계수, 결정 계수 등 수치의 크기를 비교 분석하는 것도 물론 의미가 있지만 그 전에 모델의 유의성부터 살펴보아야한다. 선형모델을 fit한 것의 summary를 보면 F test 결과를 통해 모델의 유의성을 살펴볼 수 있다. PM10과 PM2.5 모두 적합된 모델이 유의하다는 결과를 보여주었다. 또한, 각 모델에 대한 residual plot을 그려 잔차들이 0을 중심으로 패턴이 없어야 하고 분산 또한 일정한지 확인하여야 한다. PM2.5는 residual plot이 선형 회귀분석 모델에서의 잔차 가정을 어느 정도 만족한다고 볼 수 있지만, PM10의 경우 잔차 값이 0과 양의 방향으로 많이 벗어난 점들이 많아 잔차 가정이 만족된다고 보기 어렵다. 따라서 선형회귀분석은 이 분석에서 적합하지 않으므로 더 좋은 모델을 적합하는 것도 고려해보아야 한다. 더 좋은 모델을 찾는 것은 future data에 대해 prediction 값이 더 정확해질 가능성을 내포하므로 의미가 있다. 관점좋은 모델을 찾는 과정은 데이터마이닝적으로 접근하면 기존 데이터를 train:test = 7:3 정도로 random split하여 training data를 이용해 다양한 모델을 학습시키고 test data를 통해 얻어지는 test MSE 값이 가장 낮은 모델을 찾는 과정이다. 여기서 다양한 모델은 loess나 natural splines, polynomial regression 등이 있을 수 있으며 tuning parameter가 존재하는 경우 cross-validation을 통해 best parameter을 찾아야 하는 과정이 수반될 것이다.


