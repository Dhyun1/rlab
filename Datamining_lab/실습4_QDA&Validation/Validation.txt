# 6-7주차 실습 : Validation

# 1. import library

install.packages("glmnet")
install.packages("MASS")
install.packages("leaps")
install.packages("AUC")
install.packages("ISLR")
install.packages("boot")

library(glmnet)
library(MASS)
library(leaps)
library(AUC)
library(ISLR)
library(boot) 

## Three validation approaches

## 1) The Validation Set Approach

# data description
data(Auto)
summary(Auto)

set.seed(1)
train <- sample(1:nrow(Auto),nrow(Auto)/2)

attach(Auto)

# Linear : Validation MSE = 23.26601
lm.fit <- lm(mpg~horsepower,data=Auto,subset=train)
mean((mpg-predict(lm.fit,Auto))[-train]^2)

# Second order : Validation MSE = 18.71
lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)

# Third order : Validation MSE = 18.79
lm.fit3=lm(mpg~poly(horsepower,3),data=Auto,subset=train)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)

# Second validation approach (sensitive)
set.seed(2)
train <- sample(1:nrow(Auto),nrow(Auto)/2)

# Linear 25.72651
lm.fit=lm(mpg~horsepower,subset=train)
mean((mpg-predict(lm.fit,Auto))[-train]^2)

# Second order 20.43036
lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)

# Third order 20.38533
lm.fit3=lm(mpg~poly(horsepower,3),data=Auto,subset=train)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)

# 2) Leave-One-Out Cross-Validation (LOOCV)

glm.fit <- glm(mpg~horsepower,data=Auto)
coef(glm.fit)

lm.fit <- lm(mpg~horsepower,data=Auto)
coef(lm.fit)

glm.fit <- glm(mpg~horsepower,data=Auto)
# default: LOOCV, K=n : 24.23151
cv.err <- cv.glm(Auto,glm.fit)
cv.err$delta

cv.error <- rep(0,5)
cv.error
for (i in 1:5){
  glm.fit <- glm(mpg~poly(horsepower,i),data=Auto)
  cv.error[i] <- cv.glm(Auto,glm.fit)$delta[1]
}
cv.error

# 3) k-Fold Cross-Validation

cv.error.10 <- rep(0,10)
cv.error.10
set.seed(17)
for (i in 1:10){
  glm.fit <- glm(mpg~poly(horsepower,i),data=Auto)
  cv.error.10[i] <- cv.glm(Auto,glm.fit,K=10)$delta[1]
}
cv.error.10

# Variable Selection - Ridge regression, Lasso regression
data(Boston)
names(Boston)

# help(Boston)
Boston <- Boston[,order(colnames(Boston))]
head(Boston)

dim(Boston)

set.seed(201910)
ind <- sample(1:nrow(Boston),ceiling(nrow(Boston)/3)) # 1:2 selection
boston_train<-Boston[-ind,]
boston_test<-Boston[ind,]

# 1. Ridge Regression
# Choosing Tuning Parameter : lambda

ridge_cv<-cv.glmnet(x = as.matrix(boston_train[,-8]),y = boston_train$medv,alpha = 0,nfolds = nrow(boston_train))

ridge_cv$lambda.min

ridge_cv$lambda.1se

plot(ridge_cv)

# Model Application : Best model -> test error
model<-glmnet(x = as.matrix(boston_train[,-8]),y = boston_train$medv,alpha = 0,lambda = ridge_cv$lambda.min)
summary(model)

model$beta
# test error (MSE)
error.ridge<-mean((predict(model, newx = as.matrix(boston_test[,-8]))-boston_test$medv)^2)
error.ridge

# 2. Lasso Regression
# Choosing Tuning Parameter : lambda

lasso_cv <- cv.glmnet(x = as.matrix(boston_train[,-8]),y = boston_train$medv,alpha = 1)

lasso_cv$lambda.min

lasso_cv$lambda.1se

GOMplot(lasso_cv)

# Model Application

model_min<-glmnet(x = as.matrix(boston_train[,-8]),y = boston_train$medv,alpha = 1,lambda = lasso_cv$lambda.min)
model_min$beta

mean((predict(model_min, newx = as.matrix(boston_test[,-8]))-boston_test$medv)^2)

model_1se<-glmnet(x = as.matrix(boston_train[,-8]),y = boston_train$medv,alpha = 1,lambda = lasso_cv$lambda.1se)
model_1se$beta

mean((predict(model_1se, newx = as.matrix(boston_test[,-8]))-boston_test$medv)^2)


# The Bootstrap

# Data : Portfolio
names(Portfolio)

str(Portfolio)

dim(Portfolio)

# 1) Estimiating alpha (refer to ISLR p.187)

# 1. bootstrap function

alpha.fn=function(data,index){
  X=data$X[index]
  Y=data$Y[index]
  return((var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y)))
}
alpha.fn

str(Portfolio)

# 2. conduct
alpha.fn(Portfolio,1:100)

set.seed(1)
alpha.fn(Portfolio,sample(100,100,replace=T))
# Conduct a bootstrap
boot(Portfolio,alpha.fn,R=1000)

# 2) Estimating the Accuracy of a Linear Regression Model

boot.fn=function(data,index)
  return(coef(lm(mpg~horsepower,data=data,subset=index)))

str(Auto)

boot.fn(Auto,1:392)

set.seed(1)
boot.fn(Auto,sample(392,392,replace=T))
boot.fn(Auto,sample(392,392,replace=T))

# Conduct a bootstrap
boot(Auto,boot.fn,1000)
# Comparision
summary(lm(mpg~horsepower,data=Auto))$coef

# bootstrap function
boot.fn=function(data,index)
  coefficients(lm(mpg~horsepower+I(horsepower^2),data=data,subset=index))

# Conduct a bootstrap
set.seed(1)
boot(Auto,boot.fn,1000)
# Comparision
summary(lm(mpg~horsepower+I(horsepower^2),data=Auto))$coef

